name: Database Backup to Google Cloud Storage

on:
  schedule:
    - cron: '0 0 * * *' # Runs daily at midnight UTC
  workflow_dispatch: # Allows manual trigger from GitHub UI

jobs:
  backup:
    name: Backup NeonDB to GCS
    runs-on: ubuntu-latest
    timeout-minutes: 30 # Prevent hanging jobs
    
    permissions:
      contents: 'read'
      id-token: 'write'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install PostgreSQL Client
      run: |
        sudo apt-get update
        sudo apt-get install -y postgresql-client

    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v2
      with:
        credentials_json: ${{ secrets.GCP_SA_KEY }}

    - name: Set up Cloud SDK
      uses: google-github-actions/setup-gcloud@v2

    - name: Create Database Dump
      id: dump
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
      run: |
        TIMESTAMP=$(date +%Y-%m-%d_%H-%M-%S)
        FILENAME="backup_${TIMESTAMP}.dump"
        echo "FILENAME=${FILENAME}" >> $GITHUB_OUTPUT
        
        # Use pg_dump to create a compressed custom-format archive
        # -F c: Custom format (compressed, allows selective restore)
        # -Z 5: Medium compression level (balance between speed and size)
        pg_dump "$DATABASE_URL" -F c -Z 5 -f "$FILENAME"
        
        echo "Dump created: $FILENAME"

    - name: Verify Backup Integrity
      run: |
        FILENAME="${{ steps.dump.outputs.FILENAME }}"
        
        # 1. Check file existence and non-zero size
        if [ ! -s "$FILENAME" ]; then
          echo "Error: Backup file is missing or empty!"
          exit 1
        fi
        
        # 2. Verify internal structure using pg_restore --list
        # This reads the table of contents to ensure the file isn't truncated/corrupt
        if ! pg_restore --list "$FILENAME" > /dev/null; then
          echo "Error: Backup file is corrupt (pg_restore check failed)!"
          exit 1
        fi
        
        echo "Backup integrity verified successfully."

    - name: Upload to Google Cloud Storage
      run: |
        FILENAME="${{ steps.dump.outputs.FILENAME }}"
        BUCKET="${{ secrets.GCP_BACKUP_BUCKET }}"
        
        # Upload the dump file
        gcloud storage cp "$FILENAME" "gs://${BUCKET}/backups/"
        
        # Create and upload a 'LATEST' pointer file
        # This makes it easy for scripts to find the most recent backup
        echo "backups/${FILENAME}" > LATEST
        gcloud storage cp LATEST "gs://${BUCKET}/LATEST"
        
        echo "Upload complete: gs://${BUCKET}/backups/${FILENAME}"

    # Optional: Add notification step here (e.g., Slack, Discord, Email)
    # - name: Notify on Failure
    #   if: failure()
    #   run: |
    #     curl -X POST -H 'Content-type: application/json' --data '{"text":"‚ùå Database Backup Failed!"}' ${{ secrets.SLACK_WEBHOOK_URL }}
